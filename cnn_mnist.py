# -*- coding: utf-8 -*-
"""CNN_MNIST

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15okYB255D_9-Z0e2VernFT8gKuZkMFnm

# **Red Neuronal Convolucional**


---

## **Estudiante:**

Carlos Sebastián Zamora Rosero

---

## **Objetivo:**
Crear su propia red neuronal convolucional con el dataset MNIST.

la red neuronal debe tener al menos 1 callback, deben mostrar como evoluciona la funcion de perdida y el accurracy en funcion de la epoch, y por ultimo mostrar el reporte de clasificacion con los datos de prueba.


---

## **Librerías**
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
from os import listdir
import os
import pandas as pd

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

plt.figure()

plt.imshow(x_train[0,:,:], cmap="gray")
plt.grid("off")
plt.show()

plt.imshow(x_train[1,:,:], cmap="gray")
plt.grid("off")
plt.show()

plt.imshow(x_train[2,:,:], cmap="gray")
plt.grid("off")
plt.show()

"""## **Clases del dataset MNIST**"""

# Cargar el dataset MNIST
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# Encontrar las clases únicas en las etiquetas de entrenamiento
class_names = set(y_train)
print("Clases encontradas:", class_names)

clases = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']

_, axes = plt.subplots(1, 10, figsize=(15, 15))

for i in range(len(clases)):

    axes[i].imshow(x_train[y_train == i][0], cmap="gray")
    axes[i].set_title(clases[i])

    axes[i].xaxis.set_tick_params(labelbottom=False)
    axes[i].yaxis.set_tick_params(labelleft=False)
    axes[i].set_xticks([])
    axes[i].set_yticks([])

plt.show()

plt.figure()

plt.bar(clases, np.bincount(y_train))

plt.show()

"""## **Preprocesamiento**

Normalización

Dividir X por 255 es una técnica común en el preprocesamiento de imágenes llamada normalización o escalado.

255 es el valor máximo de un píxel en una imagen en escala de grises o en cada canal de una imagen a color. Esto se debe a que las imágenes generalmente están representadas en un formato de 8 bits por canal de color, donde cada píxel tiene un valor entre 0 y 255.

          x_train_scaled = x_train / 255.0
          x_test_scaled = x_test / 255.0

# **Modelo**

Para la red neuronal se crean 4 capas de convolución y 2 capas de pooling. Se utiliza padding en las capas convolucionales para mantener las dimensiones espaciales de las imágenes y evitar una reducción excesiva del tamaño en cada capa, porque cuando realicé la ejecución sin padding se reducian demasiado las dimeniones. Se emplea BatchNormalization para estabilizar y acelerar el entrenamiento, normalizando las activaciones y los gradientes. Además, se incorpora Dropout para reducir el riesgo de sobreajuste, desactivando aleatoriamente un porcentaje de neuronas durante el entrenamiento. La función de activación ReLU se utiliza por su simplicidad y efectividad en la introducción de no linealidad en el modelo, lo que permite aprender características complejas sin enfrentar problemas de desvanecimiento del gradiente.
"""

from sklearn.model_selection import train_test_split
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

from sklearn.model_selection import train_test_split
import tensorflow as tf

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

x_train_scaled = x_train / 255.0
x_test_scaled = x_test / 255.0

x_train_scaled = x_train_scaled.reshape(-1, 28, 28, 1)
x_test_scaled = x_test_scaled.reshape(-1, 28, 28, 1)

X_train, X_val, y_train, y_val = train_test_split(x_train_scaled, y_train, test_size=0.2, stratify=y_train)

modelo = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
    tf.keras.layers.MaxPooling2D(2, 2),  # pooling después de la segunda capa convolucional
    tf.keras.layers.Dropout(0.3),

    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Dropout(0.4),

    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.5),


    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.5),


    tf.keras.layers.Dense(10, activation='softmax')
])



modelo.summary()

tf.keras.utils.plot_model(modelo, show_shapes=True)

"""Se aplican transformaciones para mejorar el sobreajuste."""

train_data_augmenter = tf.keras.preprocessing.image.ImageDataGenerator(
    #rescale=1/255.0,
    rotation_range=10,
    shear_range=0.1,
    zoom_range=0.1,
    width_shift_range=0.1,
    height_shift_range=0.1,
    validation_split=0.25
)

"""Compilación del modelo."""

modelo.compile(optimizer="adam",
               loss='sparse_categorical_crossentropy',
               metrics=['accuracy'])

"""## **Callbacks.**

Se utilizan los callbacks de EarlyStopping y ReduceLROnPlateau.

EarlyStopping ayuda a detener el entrenamiento antes de tiempo si el modelo deja de mejorar en el conjunto de validación. Y ReduceLROnPlateau ajusta la tasa de aprendizaje cuando el modelo deja de mejorar.
"""

CB = [
    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=5, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)
    #tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True),

]

"""## **Evolución de la funcion de perdida y el accurracy en función de la epoch**

Entrenamiento utilizando train_data_augmenter
"""

history = modelo.fit(
    train_data_augmenter.flow(X_train, y_train, subset="training"),
    validation_data=train_data_augmenter.flow(X_train, y_train, subset="validation"),
    epochs=30,
    callbacks=CB,
    batch_size=32,
    validation_batch_size=32
)

"""X_train, X_val, y_train, y_val = train_test_split(x_train_scaled, y_train, test_size=0.2, stratify=y_train)

## **Evolución de la exactitud y de la función de pérdida**
"""

history_dict = history.history
epochs = range(1, len(history_dict["accuracy"]) + 1)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

ax1.plot(epochs, history_dict["accuracy"], label="Train Accuracy")
ax1.plot(epochs, history_dict["val_accuracy"], label="Validation Accuracy")
ax1.set_title("Evolución de la Exactitud")
ax1.set_xlabel("Epoch")
ax1.set_ylabel("Accuracy")
ax1.legend()

ax2.plot(epochs, history_dict["loss"], label="Train Loss")
ax2.plot(epochs, history_dict["val_loss"], label="Validation Loss")
ax2.set_title("Evolución de la Función de Pérdida")
ax2.set_xlabel("Epoch")
ax2.set_ylabel("Loss")
ax2.legend()

plt.show()

"""## **Reporte de clasificacion con los datos de prueba y entrenamiento**"""

from sklearn.metrics import classification_report, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

clases = list(range(10))

print("\nMÉTRICAS:\n")
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

print("\nENTRENAMIENTO Y VALIDACIÓN:\n")

y_pred_train = np.argmax(modelo.predict(X_train), axis=1)

print(classification_report(y_train, y_pred_train))

# matriz de confusión train
ConfusionMatrixDisplay.from_predictions(
    y_true=y_train,
    y_pred=y_pred_train,
    display_labels=clases,
    ax=ax1
)
ax1.set_title("Entrenamiento")

print("\nPrueba:\n")

y_pred_test = np.argmax(modelo.predict(X_val), axis=1)

print(classification_report(y_val, y_pred_test))

# matriz de confusión test
ConfusionMatrixDisplay.from_predictions(
    y_true=y_val,
    y_pred=y_pred_test,
    display_labels=clases,
    ax=ax2
)
ax2.set_title("Prueba")

# Mostrar las gráficas
plt.show()

"""## **EXTRA**

### **Capas convolucionales**
"""

conv_layers = [layer for layer in modelo.layers if isinstance(layer, tf.keras.layers.Conv2D)]

np.random.seed(1)

_, axes = plt.subplots(len(conv_layers), 5, figsize=(15, len(conv_layers) * 3))

for i, layer in enumerate(conv_layers):
    axes[i, 0].set_ylabel(f"Capa conv. {i + 1}", rotation=0, labelpad=20)
    axes[i, 0].yaxis.label.set_color('black')

    filters, _ = layer.get_weights()

    filters = (filters - filters.min()) / (filters.max() - filters.min())

    sel = np.random.randint(0, filters.shape[-1], 5)
    filters_selected = filters[:, :, :, sel]
    filters_selected = np.moveaxis(filters_selected, -1, 0)

    filters_selected = np.mean(filters_selected, axis=3, keepdims=True)

    for j, filter in enumerate(filters_selected):

        axes[i, j].imshow(filter.squeeze(), cmap="gray")

        axes[i, j].set_xticks([])
        axes[i, j].set_yticks([])

    if i == 0:
        for j in range(5):
            axes[i, j].set_title(f"Filtro {j + 1}")

plt.tight_layout()
plt.show()